# !/usr/local/lib/python2.7 python 
# -*- coding=utf-8 -*-  

# bridging the python 2 and python 3 gap
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function


import os # saving files
import numpy as np # for matrix math
import matplotlib.pyplot as plt # plot and visualizing data 
import tensorflow as tf # machine learning
from data_utils import shuffle, iter_data # analyzing data 
from tqdm import tqdm # progress bar
import scipy.io as sio # open matlab ".mat" data files
import networks as net # neural networks architectures
import constraints 
import post_processing

# Load data generated by the pervious step of CrystalGAN

AHB_g = np.load('STEP1+STEP2_CrsytalGAN/'+'AHB_g.npy', 'r')
BHA_g = np.load('STEP1+STEP2_CrsytalGAN/'+'BHA_g.npy', 'r')

## some hyper parameters

n_epoch = 100
batch_size  = 35
input_dim = 3
latent_dim = 3
eps_dim = 3

# discriminator
n_layer_disc = 5
n_hidden_disc = 100

# generator
n_layer_gen = 5
n_hidden_gen= 100

# second generator
n_layer_inf = 5
n_hidden_inf= 100

## create directory for results
result_dir = 'STEP3_CrystalGAN/'
directory = result_dir
if not os.path.exists(directory):
    os.makedirs(directory)

# true samples of x and z
# Pd and Ni in this case 
AH_dataset = AHB_g
BH_dataset = BHA_g

# tf slim to define complex networks easily & quickly
slim = tf.contrib.slim 

#Create a new graph which compute the targets from the replaced Tensors.
graph_replace = tf.contrib.graph_editor.graph_replace

# Disable some tensorflow warnings !       
os.environ["TF_CPP_MIN_LOG_LEVEL"]="2"


### Construct model and training ops
tf.reset_default_graph()

# data 1 input
x_AH = tf.placeholder(tf.float32, shape=([35, 4, 18, 3]))
# data 2 input
y_BH = tf.placeholder(tf.float32, shape=([35, 4, 18, 3]))




# 2 generators - encoders 

AHB = net.Generator_AHB(y_BH, input_dim , n_layer_gen, n_hidden_gen, eps_dim, None) 
BHA = net.Generator_BHA(x_AH, latent_dim, n_layer_inf, n_hidden_inf, eps_dim, None) 


# 2 discriminators
decoder_logit_x = net.Discriminator_BH(AHB, n_layers=n_layer_disc, n_hidden=n_hidden_disc)
encoder_logit_x = graph_replace(decoder_logit_x, {AHB: x_AH}) 
decoder_logit_y = net.Discriminator_AH(BHA, n_layers=n_layer_disc, n_hidden=n_hidden_disc)
encoder_logit_y = graph_replace(decoder_logit_y, {BHA: y_BH}) 

# Compute softplus 
# to calculate loss

encoder_sigmoid_x = tf.nn.softplus(encoder_logit_x)
decoder_sigmoid_x = tf.nn.softplus(decoder_logit_x)
encoder_sigmoid_y = tf.nn.softplus(encoder_logit_y)
decoder_sigmoid_y = tf.nn.softplus(decoder_logit_y)


## Loss functions

# loss for both discriminators
decoder_loss = decoder_sigmoid_x + decoder_sigmoid_y
encoder_loss = encoder_sigmoid_x + encoder_sigmoid_y

# combined loss for discriminators

disc_loss = tf.reduce_mean(encoder_loss) - tf.reduce_mean(decoder_loss)

# 2 more generators - decoders

rec_AH = net.Generator_BHA(AHB, latent_dim, n_layer_inf, n_hidden_inf, eps_dim, True)
rec_BH = net.Generator_AHB(BHA, input_dim , n_layer_gen, n_hidden_gen,  eps_dim, True)


# Reconstruction loss 
# L_R_AH
cost_AH = tf.reduce_mean(tf.pow(rec_AH - y_BH, 2))

#L_R_BH
cost_BH = tf.reduce_mean(tf.pow(rec_BH - x_AH, 2))

adv_loss = tf.reduce_mean(decoder_loss) # + tf.reduce_mean( encoder_loss )
gen_loss = 1*adv_loss + 1.*cost_BH  + 1.*cost_AH

vars_x = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,   "inference")
vars_y = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,   "generative")
dvars_x = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "discriminator_x")
dvars_y = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "discriminator_y")
opt = tf.train.AdamOptimizer(1e-4, beta1=0.5)
train_gen_op =  opt.minimize(gen_loss, var_list=vars_x + vars_y)
train_disc_op = opt.minimize(disc_loss, var_list=dvars_x + dvars_y)




##############################################################################################
# TRAINING and results
##############################################################################################
sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())


FG = []
FD = []
F_GEO = []

for epoch in tqdm(range(n_epoch), total=n_epoch):
	
	BHA_gen = np.array([]);
	AHB_gen = np.array([]);
	AH_dataset, BH_dataset= shuffle(AH_dataset, BH_dataset)

	for x, y in iter_data(AH_dataset, BH_dataset, size=batch_size):
		for _ in range(1):
			f_d, _ = sess.run([disc_loss, train_disc_op], feed_dict={x_AH: x, y_BH:y})
		for _ in range(5):
			f_g, _ = sess.run([[adv_loss, cost_BH, cost_AH], train_gen_op], feed_dict={x_AH: x, y_BH:y})
		FG.append(f_g)
		FD.append(f_d)


	for x, y in iter_data(AH_dataset, BH_dataset, size=batch_size):

		
		temp_BHA_gen = sess.run(BHA, feed_dict={x_AH: x, y_BH:y})
		BHA_gen = np.vstack([BHA_gen, temp_BHA_gen]) if BHA_gen.size else temp_BHA_gen
		BHA_gen = BHA_gen.tolist()
		BHA_ = post_processing.clean_data(BHA_gen)
		post_processing.write_POSCAR(directory,BHA_)
		neighbors = constraints.all_neighbors(result_dir,1.8)
		print(neighbors)
		constraint_d1, constraint_d2 = constraints.geo_constraints(neighbors)
		geo_cost, geo_cost_op = constraints.geo_loss(constraint_d1, constraint_d2 )
		f_geo, _ = sess.run([geo_cost,geo_cost_op], feed_dict = {x_AH : x,y_BH : y })

		temp_AHB_gen = sess.run(AHB, feed_dict={x_AH: x, y_BH:y})
		AHB_gen = np.vstack([AHB_gen, temp_AHB_gen]) if AHB_gen.size else temp_AHB_gen
		AHB_gen = AHB_gen.tolist()
		AHB_ = post_processing.clean_data(AHB_gen)
		post_processing.write_POSCAR(directory,AHB_)
		neighbors = constraints.all_neighbors(result_dir,1.8)
		print(neighbors)
		constraint_d1, constraint_d2 = constraints.geo_constraints(neighbors)
		geo_cost, geo_cost_op = constraints.geo_loss(constraint_d1, constraint_d2 )
		f_geo, _ = sess.run([geo_cost,geo_cost_op], feed_dict = {x_AH : x,y_BH : y })

	F_GEO.append(f_geo)
	writer = tf.summary.FileWriter('./graphs',sess.graph)
	writer.close()   

##--------------------------------------------
## ---------- learning curves ----------------
##--------------------------------------------
# plot loss per iteration 
# for generators, discriminators and constraints 
fig_curve, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))
ax.plot(FD, label="Discriminator")
ax.plot(np.array(FG)[:,0], label="Generator")
ax.plot(np.array(FG)[:,1], label="Reconstruction AH")
ax.plot(np.array(FG)[:,2], label="Reconstruction BH")
ax.plot(F_GEO, label="Geometric Loss")
#ax.set_yscale('log')
plt.ylabel('Loss')
plt.xlabel('Iteration')
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
ax.axis('on')
plt.show()